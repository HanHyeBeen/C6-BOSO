//
//  STTTranscriberManager.swift
//  Openock
//
//  Created by JiJooMaeng on 10/26/25.
//

/*
 STT Transcriber Manager

 Abstract:
 Manages Speech-to-Text transcription using macOS 26's SpeechTranscriber API.
 Handles audio format conversion, analyzer pipeline, and transcript generation.
 */

import FoundationModels
import Foundation
import Speech
import AVFoundation
import Combine

class STTTranscriberManager: ObservableObject {

  @Published var transcript = ""
  @Published var errorMessage: String?

  private var transcriber: SpeechTranscriber?
  private var analyzer: SpeechAnalyzer?
  private var inputContinuation: AsyncStream<AnalyzerInput>.Continuation?
  private var analyzerFormat: AVAudioFormat?
  private var converter: AVAudioConverter?

  @Published var isTranscribing = false

  // Foundation Models for text improvement
  private var enableAIImprovement = true
  private var debugMode = true  // ë””ë²„ê·¸ ëª¨ë“œ: STT ì›ë³¸ë„ í•¨ê»˜ í‘œì‹œ
  private var recentContextSentences: [String] = []  // ìµœê·¼ ë¬¸ì¥ë“¤ (ë§¥ë½ìš©)
  private let maxContextSentences = 5  // ìµœëŒ€ 5ê°œ ë¬¸ì¥ ìœ ì§€ (ë” ë§ì€ ë§¥ë½)

  /// Start the transcription process
  @MainActor
  func startTranscription() async {
    print("ğŸ”„ [STTTranscriberManager] Starting transcription...")

    // Create SpeechTranscriber
    transcriber = SpeechTranscriber(
        locale: Locale(identifier: "ko-KR"),
        preset: .progressiveTranscription
    )
    print("âœ… [STTTranscriberManager] SpeechTranscriber created")

    guard let transcriber = transcriber else {
      print("âŒ [STTTranscriberManager] No transcriber available")
      return
    }

    // Assets
    if let installationRequest = try? await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {
        try? await installationRequest.downloadAndInstall()
    }

    // Initialize Foundation Models for AI text improvement
    if #available(macOS 15.1, *), enableAIImprovement {
      do {
        try await STTFoundationModels.shared.initialize()
        print("âœ… [STTTranscriberManager] Foundation Models initialized for text improvement")
      } catch {
        print("âš ï¸ [STTTranscriberManager] Foundation Models initialization failed: \(error)")
        enableAIImprovement = false
      }
    }

    // Set up analyzer pipeline
    let analyzer = SpeechAnalyzer(modules: [transcriber])
    self.analyzer = analyzer

    let bestFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber])
    self.analyzerFormat = bestFormat

    if let bestFormat = bestFormat {
      print("âœ… [STTTranscriberManager] Best analyzer format: \(bestFormat.sampleRate)Hz, \(bestFormat.channelCount) channels")
    } else {
      print("âš ï¸ [STTTranscriberManager] No best format available")
    }

    // Create AsyncStream
    let (inputSequence, inputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)
    self.inputContinuation = inputBuilder

    // Start analyzer
    Task {
      print("ğŸ”„ [STTTranscriberManager] Starting analyzer...")
      do {
        try await analyzer.start(inputSequence: inputSequence)
        print("âœ… [STTTranscriberManager] Analyzer started")
      } catch {
        print("âŒ [STTTranscriberManager] Analyzer start error: \(error)")
      }
    }

    // Process transcription results in background
    isTranscribing = true
    Task {
      await processTranscriptionResults(transcriber: transcriber)
    }

    print("âœ… [STTTranscriberManager] Transcription started (background processing)")
  }

  /// Process transcription results from SpeechTranscriber
  @MainActor
  private func processTranscriptionResults(transcriber: SpeechTranscriber) async {
    var finalized = AttributedString("")
    var volatile = AttributedString("")

    print("ğŸ”„ [STTTranscriberManager] Waiting for transcription results...")
    var resultCount = 0

    do {
      for try await result in transcriber.results {
        resultCount += 1
        print("ğŸ“ [STTTranscriberManager] Result #\(resultCount) - isFinal: \(result.isFinal), text length: \(result.text.characters.count)")

        if result.isFinal {
          let originalText = String(result.text.characters)

          // ë””ë²„ê·¸: ì›ë³¸ STT ê²°ê³¼ ì¶œë ¥
          print("ğŸ¤ [STTTranscriberManager] STT ì›ë³¸: '\(originalText)'")

          // Foundation Modelsë¡œ í…ìŠ¤íŠ¸ ê°œì„  (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
          let improvedText: String
          if #available(macOS 15.1, *), enableAIImprovement, !originalText.isEmpty {
            // íƒ€ì„ì•„ì›ƒ 5ì´ˆ ì„¤ì •
            improvedText = await withTimeout(seconds: 5) {
              do {
                // ìµœê·¼ 5ë¬¸ì¥ì˜ ë§¥ë½ì„ ì „ë‹¬
                let contextString = self.recentContextSentences.isEmpty ? nil : self.recentContextSentences.joined(separator: " ")

                let result = try await STTFoundationModels.shared.improveText(
                  originalText,
                  previousContext: contextString
                )

                // ë³€ê²½ ì‚¬í•­ í‘œì‹œ
                if result != originalText {
                  print("âœ¨ [STTTranscriberManager] AI êµì •: '\(originalText)' â†’ '\(result)'")
                } else {
                  print("âœ… [STTTranscriberManager] AI íŒë‹¨: ìˆ˜ì • ë¶ˆí•„ìš”")
                }
                return result
              } catch {
                print("âš ï¸ [STTTranscriberManager] AI improvement failed: \(error)")
                return originalText
              }
            } ?? originalText  // íƒ€ì„ì•„ì›ƒ ì‹œ ì›ë³¸ ì‚¬ìš©
          } else {
            improvedText = originalText
            print("â­ï¸ [STTTranscriberManager] AI êµì • ë¹„í™œì„±í™”ë¨")
          }

          // ë””ë²„ê·¸ ëª¨ë“œ: ì›ë³¸ê³¼ ê°œì„ ë³¸ì„ í•¨ê»˜ í‘œì‹œ
          if debugMode && improvedText != originalText {
            finalized += AttributedString("[ì›ë³¸: \(originalText)] \(improvedText)\n")
          } else {
            finalized += AttributedString(improvedText)
          }

          // ìµœê·¼ ë§¥ë½ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 5ë¬¸ì¥)
          recentContextSentences.append(improvedText)
          if recentContextSentences.count > maxContextSentences {
            recentContextSentences.removeFirst()
          }

          volatile = AttributedString("")
          print("ğŸ“ [STTTranscriberManager] ìµœì¢… ì¶œë ¥: '\(improvedText)'")
        } else {
          // Partial ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ í‘œì‹œ (ì‹¤ì‹œê°„ì„± ìœ ì§€)
          volatile = result.text
          print("â³ [STTTranscriberManager] Partial text: '\(String(result.text.characters))'")
        }

        let newTranscript = String(finalized.characters) + String(volatile.characters)
        self.objectWillChange.send()
        self.transcript = newTranscript
        print("âœ… [STTTranscriberManager] Transcript updated (length \(newTranscript.count))")
      }
      print("âš ï¸ [STTTranscriberManager] Transcription loop ended")
    } catch {
      print("âŒ [STTTranscriberManager] Transcription error: \(error.localizedDescription)")
      self.objectWillChange.send()
      self.errorMessage = "ì „ì‚¬ ì˜¤ë¥˜: \(error.localizedDescription)"
    }

    isTranscribing = false
  }

  /// Process audio buffer and send to transcriber
  func processAudio(buffer: AVAudioPCMBuffer) {
    guard isTranscribing else {
      print("âš ï¸ [STTTranscriberManager] Not transcribing, ignoring buffer")
      return
    }

    guard let analyzerFormat = analyzerFormat else {
      print("âš ï¸ [STTTranscriberManager] No analyzer format, ignoring buffer")
      return
    }

    print("ğŸ¤ [STTTranscriberManager] Received audio buffer: \(buffer.frameLength) frames at \(buffer.format.sampleRate)Hz, \(buffer.format.channelCount) channels")

    // Convert format if needed
    let sendBuffer: AVAudioPCMBuffer

    let needsConversion = buffer.format.sampleRate != analyzerFormat.sampleRate ||
                         buffer.format.channelCount != analyzerFormat.channelCount

    if needsConversion {
      print("ğŸ”„ [STTTranscriberManager] Converting from \(buffer.format.sampleRate)Hz/\(buffer.format.channelCount)ch to \(analyzerFormat.sampleRate)Hz/\(analyzerFormat.channelCount)ch")

      if converter == nil || converter?.inputFormat != buffer.format || converter?.outputFormat != analyzerFormat {
        converter = AVAudioConverter(from: buffer.format, to: analyzerFormat)
        converter?.primeMethod = .none
        print("âœ… [STTTranscriberManager] Created new converter")
      }

      guard let converter = converter else {
        print("âŒ [STTTranscriberManager] Failed to create converter")
        return
      }

      // Calculate output frame capacity
      let outputFrameCapacity = AVAudioFrameCount(ceil(Double(buffer.frameLength) * analyzerFormat.sampleRate / buffer.format.sampleRate))

      guard let out = AVAudioPCMBuffer(pcmFormat: analyzerFormat, frameCapacity: outputFrameCapacity) else {
        print("âŒ [STTTranscriberManager] Failed to create output buffer")
        return
      }

      var err: NSError?
      let status = converter.convert(to: out, error: &err) { _, inStatus in
        inStatus.pointee = .haveData
        return buffer
      }

      if let err = err {
        print("âŒ [STTTranscriberManager] AVAudioConverter error: \(err)")
        return
      }

      if status == .error {
        print("âŒ [STTTranscriberManager] Conversion failed with error status")
        return
      }

      // Verify frameLength
      guard out.frameLength > 0 else {
        print("âŒ [STTTranscriberManager] Converted buffer has zero frames")
        return
      }

      print("âœ… [STTTranscriberManager] Converted successfully: \(out.frameLength) frames")
      sendBuffer = out
    } else {
      print("âœ… [STTTranscriberManager] No conversion needed, using original buffer")
      sendBuffer = buffer
    }

    // Send to analyzer
    inputContinuation?.yield(AnalyzerInput(buffer: sendBuffer))
    print("âœ… [STTTranscriberManager] Audio buffer sent to analyzer (\(sendBuffer.frameLength) frames)")
  }

  /// Stop transcription
  func stopTranscription() {
    print("ğŸ›‘ [STTTranscriberManager] Stopping transcription...")

    inputContinuation?.finish()
    inputContinuation = nil
    analyzer = nil
    analyzerFormat = nil
    converter = nil
    transcriber = nil
    isTranscribing = false
    recentContextSentences.removeAll()

    // Cleanup Foundation Models
    if #available(macOS 15.1, *) {
      STTFoundationModels.shared.cleanup()
    }

    print("âœ… [STTTranscriberManager] Transcription stopped")
  }

  /// Clear transcript
  func clearTranscript() {
    transcript = ""
    errorMessage = nil
    recentContextSentences.removeAll()
  }

  /// AI í…ìŠ¤íŠ¸ ê°œì„  ê¸°ëŠ¥ ì¼œê¸°/ë„ê¸°
  func setAIImprovement(enabled: Bool) {
    enableAIImprovement = enabled
    print("ğŸ”§ [STTTranscriberManager] AI improvement \(enabled ? "enabled" : "disabled")")
  }

  /// ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°/ë„ê¸°
  func setDebugMode(enabled: Bool) {
    debugMode = enabled
    print("ğŸ”§ [STTTranscriberManager] Debug mode \(enabled ? "enabled" : "disabled")")
  }

  deinit {
    stopTranscription()
  }

  /// Timeout helper
  private func withTimeout<T>(seconds: TimeInterval, operation: @escaping () async -> T) async -> T? {
    await withTaskGroup(of: T?.self) { group in
      group.addTask {
        await operation()
      }

      group.addTask {
        try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
        return nil
      }

      let result = await group.next()
      group.cancelAll()
      return result ?? nil
    }
  }
}
