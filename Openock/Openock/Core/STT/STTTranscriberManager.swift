//
//  STTTranscriberManager.swift
//  Openock
//
//  Created by JiJooMaeng on 10/26/25.
//

/*
 STT Transcriber Manager

 Abstract:
 Manages Speech-to-Text transcription using macOS 26's SpeechTranscriber API.
 Handles audio format conversion, analyzer pipeline, and transcript generation.
 */

import FoundationModels
import Foundation
import Speech
import AVFoundation
import Combine

class STTTranscriberManager: ObservableObject {

  @Published var transcript = ""
  @Published var errorMessage: String?
  @Published var detectedLanguage: String = "ko-KR"  // í˜„ì¬ ê°ì§€ëœ ì–¸ì–´

  // Korean transcriber setup
  private var transcriber: SpeechTranscriber?
  private var analyzer: SpeechAnalyzer?
  private var inputContinuation: AsyncStream<AnalyzerInput>.Continuation?
  private var analyzerFormat: AVAudioFormat?
  private var converter: AVAudioConverter?

  @Published var isTranscribing = false

  // Foundation Models for text improvement --------------------------------------------------------------
  private var enableAIImprovement = true
  private var debugMode = false  // ë””ë²„ê·¸ ëª¨ë“œ: STT ì›ë³¸ë„ í•¨ê»˜ í‘œì‹œ
  // EngTextFilter ì—¬ë¶€
  private var enableEnglishFiltering = true  // ì˜ì–´ í•„í„°ë§ í™œì„±í™” ì—¬ë¶€

  private var recentContextSentences: [String] = []  // ìµœê·¼ ë¬¸ì¥ë“¤ (ë§¥ë½ìš©)
  private let maxContextSentences = 5  // ìµœëŒ€ 5ê°œ ë¬¸ì¥ ìœ ì§€ (ë” ë§ì€ ë§¥ë½)

  /// Start the transcription process
  @MainActor
  func startTranscription() async {
    print("ğŸ”„ [STTTranscriberManager] Starting Korean transcription...")

    // âœ… Set isTranscribing early to accept incoming audio buffers
    isTranscribing = true
    print("âœ… [STTTranscriberManager] isTranscribing set to TRUE")

    // Create Korean SpeechTranscriber
    transcriber = SpeechTranscriber(
        locale: Locale(identifier: "ko-KR"),
        preset: .progressiveTranscription
    )
    print("âœ… [STTTranscriberManager] Korean transcriber created")

    guard let transcriber = transcriber else {
      print("âŒ [STTTranscriberManager] Failed to create transcriber")
      return
    }

    // Download assets for Korean
    if let installRequest = try? await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {
        try? await installRequest.downloadAndInstall()
        print("âœ… [STTTranscriberManager] Korean assets downloaded")
    }

    // Initialize Foundation Models for AI text improvement
    if enableAIImprovement {
      do {
        try await STTFoundationModels.shared.initialize()
        print("âœ… [STTTranscriberManager] Foundation Models initialized for text improvement")
      } catch {
        print("âš ï¸ [STTTranscriberManager] Foundation Models initialization failed: \(error)")
        enableAIImprovement = false
      }
    }

    // Set up analyzer pipeline for Korean
    let analyzer = SpeechAnalyzer(modules: [transcriber])
    self.analyzer = analyzer

    // Get best format for Korean transcriber
    let bestFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber])
    self.analyzerFormat = bestFormat

    if let bestFormat = bestFormat {
      print("âœ… [STTTranscriberManager] Best analyzer format: \(bestFormat.sampleRate)Hz, \(bestFormat.channelCount) channels")
    } else {
      print("âš ï¸ [STTTranscriberManager] No best format available")
    }

    // Create AsyncStream for Korean
    let (inputSequence, inputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)
    self.inputContinuation = inputBuilder

    print("ğŸ”„ [STTTranscriberManager] Starting analyzer in background task...")

    // Start analyzer in background (non-blocking)
    Task {
      print("ğŸ”„ [STTTranscriberManager] Starting Korean analyzer...")
      do {
        try await analyzer.start(inputSequence: inputSequence)
        print("âœ… [STTTranscriberManager] Korean analyzer started")
      } catch {
        print("âŒ [STTTranscriberManager] Korean analyzer start error: \(error)")
      }
    }

    // Process transcription results in background
    Task {
      await processTranscriptionResults(transcriber: transcriber)
    }

    // Give analyzer a moment to initialize before returning
    // This ensures continuation is ready to receive audio buffers
    try? await Task.sleep(nanoseconds: 100_000_000) // 0.1 second

    print("âœ… [STTTranscriberManager] Korean transcription setup complete (analyzer starting in background)")
  }

  /// Process transcription results from SpeechTranscriber
  @MainActor
  private func processTranscriptionResults(transcriber: SpeechTranscriber) async {
    var finalized = AttributedString("")
    var volatile = AttributedString("")

    print("ğŸ”„ [STTTranscriberManager] Waiting for transcription results...")
    var resultCount = 0

    do {
      for try await result in transcriber.results {
        resultCount += 1
        print("ğŸ“ [STTTranscriberManager] Result #\(resultCount) - isFinal: \(result.isFinal), text length: \(result.text.characters.count)")

        if result.isFinal {
          let rawOriginalText = String(result.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)

          // Filter out English characters if enabled
          let originalText: String
          if enableEnglishFiltering {
            originalText = EngTextFilter.shared.filterKoreanOnly(rawOriginalText)

            // Skip if text is empty after filtering
            guard !originalText.isEmpty else {
              print("â­ï¸ [STTTranscriberManager] Filtered text is empty, skipping")
              volatile = AttributedString("")
              let newTranscript = String(finalized.characters)
              self.objectWillChange.send()
              self.transcript = newTranscript
              continue
            }

            print("ğŸ¤ [STTTranscriberManager] STT ì›ë³¸: '\(rawOriginalText)' â†’ í•„í„°ë§: '\(originalText)'")
          } else {
            originalText = rawOriginalText
            print("ğŸ¤ [STTTranscriberManager] STT ì›ë³¸: '\(originalText)'")
          }

          self.detectedLanguage = "ko-KR"

          // Foundation Modelsë¡œ í…ìŠ¤íŠ¸ ê°œì„  (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
          let rawImprovedText: String
          if enableAIImprovement, !originalText.isEmpty {
            rawImprovedText = await withTimeout(seconds: 5) {
              do {
                let contextString = self.recentContextSentences.isEmpty ? nil : self.recentContextSentences.joined(separator: " ")
                let result = try await STTFoundationModels.shared.improveText(
                  originalText,
                  previousContext: contextString
                )
                return result
              } catch {
                print("âš ï¸ [STTTranscriberManager] AI improvement failed: \(error)")
                return originalText
              }
            } ?? originalText
          } else {
            rawImprovedText = originalText
            print("â­ï¸ [STTTranscriberManager] AI êµì • ë¹„í™œì„±í™”ë¨")
          }

          // Normalize: trim & remove extra spaces for comparison
          let improvedText = rawImprovedText.trimmingCharacters(in: .whitespacesAndNewlines)
          let normalizedOriginal = originalText.trimmingCharacters(in: .whitespacesAndNewlines)
          let normalizedImproved = improvedText.trimmingCharacters(in: .whitespacesAndNewlines)

          // Check if actually changed
          let hasChanged = normalizedOriginal != normalizedImproved

          // ë³€ê²½ ì‚¬í•­ í‘œì‹œ
          if hasChanged {
            print("âœ¨ [STTTranscriberManager] AI êµì •: '\(originalText)' â†’ '\(improvedText)'")
          } else {
            print("âœ… [STTTranscriberManager] AI íŒë‹¨: ìˆ˜ì • ë¶ˆí•„ìš”")
          }

          // ë””ë²„ê·¸ ëª¨ë“œ: ì›ë³¸ê³¼ ê°œì„ ë³¸ì„ í•¨ê»˜ í‘œì‹œ
          if debugMode && hasChanged {
            finalized += AttributedString("[ì›ë³¸: \(originalText)] \(improvedText)\n")
          } else {
            finalized += AttributedString(improvedText)
          }

          // ìµœê·¼ ë§¥ë½ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 5ë¬¸ì¥)
          recentContextSentences.append(improvedText)
          if recentContextSentences.count > maxContextSentences {
            recentContextSentences.removeFirst()
          }

          volatile = AttributedString("")
          print("ğŸ“ [STTTranscriberManager] ìµœì¢… ì¶œë ¥: '\(improvedText)'")
        } else {
          // Partial ê²°ê³¼ í‘œì‹œ (ì‹¤ì‹œê°„ì„± ìœ ì§€)
          let partialText = String(result.text.characters)
          if enableEnglishFiltering {
            let filteredPartial = EngTextFilter.shared.filterKoreanOnly(partialText)
            volatile = AttributedString(filteredPartial)
            print("â³ [STTTranscriberManager] Partial text: '\(partialText)' â†’ í•„í„°ë§: '\(filteredPartial)'")
          } else {
            volatile = result.text
            print("â³ [STTTranscriberManager] Partial text: '\(partialText)'")
          }
        }

        let newTranscript = String(finalized.characters) + String(volatile.characters)
        self.objectWillChange.send()
        self.transcript = newTranscript
        print("âœ… [STTTranscriberManager] Transcript updated (length \(newTranscript.count))")
      }
      print("âš ï¸ [STTTranscriberManager] Transcription loop ended")
    } catch {
      print("âŒ [STTTranscriberManager] Transcription error: \(error.localizedDescription)")
      self.objectWillChange.send()
      self.errorMessage = "ì „ì‚¬ ì˜¤ë¥˜: \(error.localizedDescription)"
    }

    isTranscribing = false
  }

  /// Process audio buffer and send to transcriber
  func processAudio(buffer: AVAudioPCMBuffer) {
    guard isTranscribing else {
      print("âš ï¸ [STTTranscriberManager] Not transcribing (isTranscribing=\(isTranscribing)), ignoring buffer")
      return
    }

    guard let analyzerFormat = analyzerFormat else {
      print("âš ï¸ [STTTranscriberManager] No analyzer format, ignoring buffer")
      return
    }

    print("ğŸ¤ [STTTranscriberManager] Received audio buffer: \(buffer.frameLength) frames at \(buffer.format.sampleRate)Hz, \(buffer.format.channelCount) channels")

    // Convert format if needed
    let sendBuffer: AVAudioPCMBuffer

    let needsConversion = buffer.format.sampleRate != analyzerFormat.sampleRate ||
                         buffer.format.channelCount != analyzerFormat.channelCount

    if needsConversion {
      print("ğŸ”„ [STTTranscriberManager] Converting from \(buffer.format.sampleRate)Hz/\(buffer.format.channelCount)ch to \(analyzerFormat.sampleRate)Hz/\(analyzerFormat.channelCount)ch")

      if converter == nil || converter?.inputFormat != buffer.format || converter?.outputFormat != analyzerFormat {
        converter = AVAudioConverter(from: buffer.format, to: analyzerFormat)
        converter?.primeMethod = .none
        print("âœ… [STTTranscriberManager] Created new converter")
      }

      guard let converter = converter else {
        print("âŒ [STTTranscriberManager] Failed to create converter")
        return
      }

      // Calculate output frame capacity
      let outputFrameCapacity = AVAudioFrameCount(ceil(Double(buffer.frameLength) * analyzerFormat.sampleRate / buffer.format.sampleRate))

      guard let out = AVAudioPCMBuffer(pcmFormat: analyzerFormat, frameCapacity: outputFrameCapacity) else {
        print("âŒ [STTTranscriberManager] Failed to create output buffer")
        return
      }

      var err: NSError?
      let status = converter.convert(to: out, error: &err) { _, inStatus in
        inStatus.pointee = .haveData
        return buffer
      }

      if let err = err {
        print("âŒ [STTTranscriberManager] AVAudioConverter error: \(err)")
        return
      }

      if status == .error {
        print("âŒ [STTTranscriberManager] Conversion failed with error status")
        return
      }

      // Verify frameLength
      guard out.frameLength > 0 else {
        print("âŒ [STTTranscriberManager] Converted buffer has zero frames")
        return
      }

      print("âœ… [STTTranscriberManager] Converted successfully: \(out.frameLength) frames")
      sendBuffer = out
    } else {
      print("âœ… [STTTranscriberManager] No conversion needed, using original buffer")
      sendBuffer = buffer
    }

    // Send to analyzer
    inputContinuation?.yield(AnalyzerInput(buffer: sendBuffer))
    print("âœ… [STTTranscriberManager] Audio buffer sent to analyzer (\(sendBuffer.frameLength) frames)")
  }

  /// Stop transcription
  func stopTranscription() {
    print("ğŸ›‘ [STTTranscriberManager] Stopping transcription...")
    print("ğŸ“ [STTTranscriberManager] Stop called from: \(Thread.callStackSymbols[0...3])")

    // IMPORTANT: Set isTranscribing FIRST to stop accepting new audio buffers
    isTranscribing = false
    print("âŒ [STTTranscriberManager] isTranscribing set to FALSE")

    // Finish input stream
    inputContinuation?.finish()

    // Clear all resources
    inputContinuation = nil
    analyzer = nil
    analyzerFormat = nil
    converter = nil
    transcriber = nil
    recentContextSentences.removeAll()

    // Cleanup Foundation Models
    STTFoundationModels.shared.cleanup()

    print("âœ… [STTTranscriberManager] Transcription stopped, isTranscribing: \(isTranscribing)")
  }

  /// Clear transcript
  func clearTranscript() {
    transcript = ""
    errorMessage = nil
    recentContextSentences.removeAll()
  }

  /// AI í…ìŠ¤íŠ¸ ê°œì„  ê¸°ëŠ¥ ì¼œê¸°/ë„ê¸°
  func setAIImprovement(enabled: Bool) {
    enableAIImprovement = enabled
    print("ğŸ”§ [STTTranscriberManager] AI improvement \(enabled ? "enabled" : "disabled")")
  }

  /// ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°/ë„ê¸°
  func setDebugMode(enabled: Bool) {
    debugMode = enabled
    print("ğŸ”§ [STTTranscriberManager] Debug mode \(enabled ? "enabled" : "disabled")")
  }

  /// ì˜ì–´ í•„í„°ë§ ì¼œê¸°/ë„ê¸°
  func setEnglishFiltering(enabled: Bool) {
    enableEnglishFiltering = enabled
    print("ğŸ”§ [STTTranscriberManager] English filtering \(enabled ? "enabled" : "disabled")")
  }

  deinit {
    stopTranscription()
  }

  /// Timeout helper
  private func withTimeout<T>(seconds: TimeInterval, operation: @escaping () async -> T) async -> T? {
    await withTaskGroup(of: T?.self) { group in
      group.addTask {
        await operation()
      }

      group.addTask {
        try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
        return nil
      }

      let result = await group.next()
      group.cancelAll()
      return result ?? nil
    }
  }
}
