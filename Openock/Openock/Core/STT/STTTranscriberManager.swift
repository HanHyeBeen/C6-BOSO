//
//  STTTranscriberManager.swift
//  Openock
//
//  Created by JiJooMaeng on 10/26/25.
//

/*
 STT Transcriber Manager

 Abstract:
 Manages Speech-to-Text transcription using macOS 26's SpeechTranscriber API.
 Handles audio format conversion, analyzer pipeline, and transcript generation.
 */

import FoundationModels
import Foundation
import Speech
import AVFoundation
import Combine

class STTTranscriberManager: ObservableObject {

  @Published var transcript = ""
  @Published var errorMessage: String?
  @Published var detectedLanguage: String = "ko-KR"  // í˜„ì¬ ê°ì§€ëœ ì–¸ì–´

  // Dual transcriber setup (Korean + English)
  private var koTranscriber: SpeechTranscriber?
  private var enTranscriber: SpeechTranscriber?
  private var koAnalyzer: SpeechAnalyzer?
  private var enAnalyzer: SpeechAnalyzer?
  private var koInputContinuation: AsyncStream<AnalyzerInput>.Continuation?
  private var enInputContinuation: AsyncStream<AnalyzerInput>.Continuation?
  private var analyzerFormat: AVAudioFormat?
  private var converter: AVAudioConverter?

  @Published var isTranscribing = false

  // Foundation Models for text improvement --------------------------------------------------------------
  private var enableAIImprovement = true
  private var debugMode = false  // ë””ë²„ê·¸ ëª¨ë“œ: STT ì›ë³¸ë„ í•¨ê»˜ í‘œì‹œ


  private var recentContextSentences: [String] = []  // ìµœê·¼ ë¬¸ì¥ë“¤ (ë§¥ë½ìš©)
  private let maxContextSentences = 5  // ìµœëŒ€ 5ê°œ ë¬¸ì¥ ìœ ì§€ (ë” ë§ì€ ë§¥ë½)

  // Language detection --------------------------------------------------------------
  private var languageDetectionEnabled = true  // ìë™ ì–¸ì–´ ê°ì§€ í™œì„±í™”

  /// Start the transcription process
  @MainActor
  func startTranscription() async {
    print("ğŸ”„ [STTTranscriberManager] Starting dual-language transcription...")

    // âœ… Set isTranscribing early to accept incoming audio buffers
    isTranscribing = true

    // Create Korean SpeechTranscriber
    koTranscriber = SpeechTranscriber(
        locale: Locale(identifier: "ko-KR"),
        preset: .progressiveTranscription
    )
    print("âœ… [STTTranscriberManager] Korean transcriber created")

    // Create English SpeechTranscriber
    enTranscriber = SpeechTranscriber(
        locale: Locale(identifier: "en-US"),
        preset: .progressiveTranscription
    )
    print("âœ… [STTTranscriberManager] English transcriber created")

    guard let koTranscriber = koTranscriber, let enTranscriber = enTranscriber else {
      print("âŒ [STTTranscriberManager] Failed to create transcribers")
      return
    }

    // Download assets for both languages
    if let koInstallRequest = try? await AssetInventory.assetInstallationRequest(supporting: [koTranscriber]) {
        try? await koInstallRequest.downloadAndInstall()
        print("âœ… [STTTranscriberManager] Korean assets downloaded")
    }

    if let enInstallRequest = try? await AssetInventory.assetInstallationRequest(supporting: [enTranscriber]) {
        try? await enInstallRequest.downloadAndInstall()
        print("âœ… [STTTranscriberManager] English assets downloaded")
    }

    // Initialize Foundation Models for AI text improvement
    if #available(macOS 15.1, *), enableAIImprovement {
      do {
        try await STTFoundationModels.shared.initialize()
        print("âœ… [STTTranscriberManager] Foundation Models initialized for text improvement")
      } catch {
        print("âš ï¸ [STTTranscriberManager] Foundation Models initialization failed: \(error)")
        enableAIImprovement = false
      }
    }

    // Set up analyzer pipelines for both languages
    let koAnalyzer = SpeechAnalyzer(modules: [koTranscriber])
    let enAnalyzer = SpeechAnalyzer(modules: [enTranscriber])
    self.koAnalyzer = koAnalyzer
    self.enAnalyzer = enAnalyzer

    // Get best format compatible with both transcribers
    let bestFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [koTranscriber, enTranscriber])
    self.analyzerFormat = bestFormat

    if let bestFormat = bestFormat {
      print("âœ… [STTTranscriberManager] Best analyzer format: \(bestFormat.sampleRate)Hz, \(bestFormat.channelCount) channels")
    } else {
      print("âš ï¸ [STTTranscriberManager] No best format available")
    }

    // Create AsyncStreams for both languages
    let (koInputSequence, koInputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)
    let (enInputSequence, enInputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)
    self.koInputContinuation = koInputBuilder
    self.enInputContinuation = enInputBuilder

    // Start both analyzers
    Task {
      print("ğŸ”„ [STTTranscriberManager] Starting Korean analyzer...")
      do {
        try await koAnalyzer.start(inputSequence: koInputSequence)
        print("âœ… [STTTranscriberManager] Korean analyzer started")
      } catch {
        print("âŒ [STTTranscriberManager] Korean analyzer start error: \(error)")
      }
    }

    Task {
      print("ğŸ”„ [STTTranscriberManager] Starting English analyzer...")
      do {
        try await enAnalyzer.start(inputSequence: enInputSequence)
        print("âœ… [STTTranscriberManager] English analyzer started")
      } catch {
        print("âŒ [STTTranscriberManager] English analyzer start error: \(error)")
      }
    }

    // Process transcription results from both transcribers in background
    Task {
      await processDualTranscriptionResults(koTranscriber: koTranscriber, enTranscriber: enTranscriber)
    }

    print("âœ… [STTTranscriberManager] Dual-language transcription started (background processing)")
  }

  /// Process dual transcription results from both Korean and English transcribers
  @MainActor
  private func processDualTranscriptionResults(koTranscriber: SpeechTranscriber, enTranscriber: SpeechTranscriber) async {
    var finalized = AttributedString("")
    var volatile = AttributedString("")

    print("ğŸ”„ [STTTranscriberManager] Starting dual transcription result processing...")

    // Shared actor to coordinate results
    let resultCoordinator = ResultCoordinator()

    do {
      // Process both transcribers concurrently - event-driven
      await withTaskGroup(of: Void.self) { group in
        // Korean transcriber task - process immediately when result arrives
        group.addTask { @MainActor in
          do {
            for try await result in koTranscriber.results {
              print("ğŸ‡°ğŸ‡· [Korean] Result - isFinal: \(result.isFinal), text: '\(String(result.text.characters))'")

              await resultCoordinator.updateKorean(text: result.text, isFinal: result.isFinal)

              // Immediately process
              if await self.processCombinedResults(
                coordinator: resultCoordinator,
                finalized: &finalized,
                volatile: &volatile
              ) {
                let newTranscript = String(finalized.characters) + String(volatile.characters)
                self.objectWillChange.send()
                self.transcript = newTranscript
              }
            }
          } catch {
            print("âŒ [Korean] Transcription error: \(error)")
          }
        }

        // English transcriber task - process immediately when result arrives
        group.addTask { @MainActor in
          do {
            for try await result in enTranscriber.results {
              print("ğŸ‡ºğŸ‡¸ [English] Result - isFinal: \(result.isFinal), text: '\(String(result.text.characters))'")

              await resultCoordinator.updateEnglish(text: result.text, isFinal: result.isFinal)

              // Immediately process
              if await self.processCombinedResults(
                coordinator: resultCoordinator,
                finalized: &finalized,
                volatile: &volatile
              ) {
                let newTranscript = String(finalized.characters) + String(volatile.characters)
                self.objectWillChange.send()
                self.transcript = newTranscript
              }
            }
          } catch {
            print("âŒ [English] Transcription error: \(error)")
          }
        }
      }
    } catch {
      print("âŒ Dual transcription error: \(error)")
      self.errorMessage = "ì „ì‚¬ ì˜¤ë¥˜: \(error.localizedDescription)"
    }

    isTranscribing = false
  }

  /// Process combined results from both transcribers
  @MainActor
  private func processCombinedResults(
    coordinator: ResultCoordinator,
    finalized: inout AttributedString,
    volatile: inout AttributedString
  ) async -> Bool {
    let ko = await coordinator.korean
    let en = await coordinator.english

    guard let selectedResult = await selectBestResult(koResult: ko, enResult: en) else {
      return false
    }

    if selectedResult.isFinal {
      // Prevent duplicate final processing
      guard await coordinator.shouldProcessFinal() else {
        print("â­ï¸ Skipping duplicate final result")
        return false
      }
      let originalText = String(selectedResult.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)
      let detectedLang = selectedResult.language

      print("ğŸ¯ [Selected \(detectedLang)] STT ì›ë³¸: '\(originalText)'")
      self.detectedLanguage = detectedLang

      // AI improvement
      let rawImprovedText: String
      if #available(macOS 15.1, *), self.enableAIImprovement, !originalText.isEmpty {
        rawImprovedText = await self.withTimeout(seconds: 5) {
          do {
            let contextString = self.recentContextSentences.isEmpty ? nil : self.recentContextSentences.joined(separator: " ")
            return try await STTFoundationModels.shared.improveText(originalText, previousContext: contextString, language: detectedLang)
          } catch {
            print("âš ï¸ AI improvement failed: \(error)")
            return originalText
          }
        } ?? originalText
      } else {
        rawImprovedText = originalText
      }

      let improvedText = rawImprovedText.trimmingCharacters(in: .whitespacesAndNewlines)
      let hasChanged = originalText != improvedText

      if hasChanged {
        print("âœ¨ AI êµì •: '\(originalText)' â†’ '\(improvedText)'")
      } else {
        print("âœ… AI íŒë‹¨: ìˆ˜ì • ë¶ˆí•„ìš”")
      }

      // Display with language indicator
      if self.debugMode && hasChanged {
        finalized += AttributedString("[\(detectedLang == "ko-KR" ? "ğŸ‡°ğŸ‡·" : "ğŸ‡ºğŸ‡¸") ì›ë³¸: \(originalText)] \(improvedText)\n")
      } else {
        finalized += AttributedString(improvedText)
      }

      self.recentContextSentences.append(improvedText)
      if self.recentContextSentences.count > self.maxContextSentences {
        self.recentContextSentences.removeFirst()
      }

      volatile = AttributedString("")

      // Clear processed results
      await coordinator.clearBoth()

    } else {
      // Partial result
      volatile = selectedResult.text
      print("â³ Partial (\(selectedResult.language)): '\(String(selectedResult.text.characters))'")
    }

    return true
  }

  // Result coordinator actor to safely share state between tasks
  actor ResultCoordinator {
    var korean: (text: AttributedString, isFinal: Bool)?
    var english: (text: AttributedString, isFinal: Bool)?
    private var lastProcessedFinalTimestamp: Date?
    private var processingFinal = false  // Prevent duplicate final processing

    func updateKorean(text: AttributedString, isFinal: Bool) {
      korean = (text: text, isFinal: isFinal)
    }

    func updateEnglish(text: AttributedString, isFinal: Bool) {
      english = (text: text, isFinal: isFinal)
    }

    func shouldProcessFinal() -> Bool {
      // If already processing a final result, skip
      if processingFinal {
        return false
      }

      // Check if either has a final result
      let koFinal = korean?.isFinal ?? false
      let enFinal = english?.isFinal ?? false

      if koFinal || enFinal {
        processingFinal = true
        return true
      }

      return false
    }

    func clearBoth() {
      korean = nil
      english = nil
      processingFinal = false
      lastProcessedFinalTimestamp = Date()
    }
  }

  /// Select best result between Korean and English transcriptions
  private func selectBestResult(koResult: (text: AttributedString, isFinal: Bool)?, enResult: (text: AttributedString, isFinal: Bool)?) -> (text: AttributedString, isFinal: Bool, language: String)? {
    guard languageDetectionEnabled else {
      // If detection disabled, prefer Korean
      if let ko = koResult {
        return (ko.text, ko.isFinal, "ko-KR")
      }
      return nil
    }

    // If only one has result, use that
    if koResult == nil, let en = enResult {
      return (en.text, en.isFinal, "en-US")
    }
    if let ko = koResult, enResult == nil {
      return (ko.text, ko.isFinal, "ko-KR")
    }

    guard let ko = koResult, let en = enResult else {
      return nil
    }

    let koText = String(ko.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)
    let enText = String(en.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)

    // If one is empty, use the other
    if koText.isEmpty && !enText.isEmpty {
      return (en.text, en.isFinal, "en-US")
    }
    if !koText.isEmpty && enText.isEmpty {
      return (ko.text, ko.isFinal, "ko-KR")
    }

    // Both have text - compare by length (longer usually means more confident)
    let koLength = koText.count
    let enLength = enText.count

    // If one is significantly longer (>30% difference), prefer that
    let lengthRatio = Double(max(koLength, enLength)) / Double(max(min(koLength, enLength), 1))

    if lengthRatio > 1.3 {
      if koLength > enLength {
        print("ğŸ¯ [Detection] Selected Korean (length: \(koLength) vs \(enLength))")
        return (ko.text, ko.isFinal, "ko-KR")
      } else {
        print("ğŸ¯ [Detection] Selected English (length: \(enLength) vs \(koLength))")
        return (en.text, en.isFinal, "en-US")
      }
    }

    // If lengths similar, use heuristic: check for ASCII/Korean characters
    let koHasKorean = koText.contains(where: { char in
      let scalar = char.unicodeScalars.first
      return scalar.map { (0xAC00...0xD7A3).contains($0.value) } ?? false
    })

    let enHasKorean = enText.contains(where: { char in
      let scalar = char.unicodeScalars.first
      return scalar.map { (0xAC00...0xD7A3).contains($0.value) } ?? false
    })

    // Prefer Korean transcriber if Korean characters detected in either
    if koHasKorean {
      print("ğŸ¯ [Detection] Selected Korean (Korean chars detected)")
      return (ko.text, ko.isFinal, "ko-KR")
    }

    // Otherwise prefer English
    print("ğŸ¯ [Detection] Selected English (no Korean chars)")
    return (en.text, en.isFinal, "en-US")
  }

  /// Process transcription results from SpeechTranscriber (legacy, kept for reference)
  @MainActor
  private func processTranscriptionResults(transcriber: SpeechTranscriber) async {
    var finalized = AttributedString("")
    var volatile = AttributedString("")

    print("ğŸ”„ [STTTranscriberManager] Waiting for transcription results...")
    var resultCount = 0

    do {
      for try await result in transcriber.results {
        resultCount += 1
        print("ğŸ“ [STTTranscriberManager] Result #\(resultCount) - isFinal: \(result.isFinal), text length: \(result.text.characters.count)")

        if result.isFinal {
          let originalText = String(result.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)

          // ë””ë²„ê·¸: ì›ë³¸ STT ê²°ê³¼ ì¶œë ¥
          print("ğŸ¤ [STTTranscriberManager] STT ì›ë³¸: '\(originalText)'")

          // Foundation Modelsë¡œ í…ìŠ¤íŠ¸ ê°œì„  (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
          let rawImprovedText: String
          if #available(macOS 15.1, *), enableAIImprovement, !originalText.isEmpty {
            // íƒ€ì„ì•„ì›ƒ 5ì´ˆ ì„¤ì •
            rawImprovedText = await withTimeout(seconds: 5) {
              do {
                // ìµœê·¼ 5ë¬¸ì¥ì˜ ë§¥ë½ì„ ì „ë‹¬
                let contextString = self.recentContextSentences.isEmpty ? nil : self.recentContextSentences.joined(separator: " ")

                let result = try await STTFoundationModels.shared.improveText(
                  originalText,
                  previousContext: contextString
                )

                return result
              } catch {
                print("âš ï¸ [STTTranscriberManager] AI improvement failed: \(error)")
                return originalText
              }
            } ?? originalText  // íƒ€ì„ì•„ì›ƒ ì‹œ ì›ë³¸ ì‚¬ìš©
          } else {
            rawImprovedText = originalText
            print("â­ï¸ [STTTranscriberManager] AI êµì • ë¹„í™œì„±í™”ë¨")
          }

          // Normalize: trim & remove extra spaces for comparison
          let improvedText = rawImprovedText.trimmingCharacters(in: .whitespacesAndNewlines)
          let normalizedOriginal = originalText.trimmingCharacters(in: .whitespacesAndNewlines)
          let normalizedImproved = improvedText.trimmingCharacters(in: .whitespacesAndNewlines)

          // Check if actually changed
          let hasChanged = normalizedOriginal != normalizedImproved

          // ë³€ê²½ ì‚¬í•­ í‘œì‹œ
          if hasChanged {
            print("âœ¨ [STTTranscriberManager] AI êµì •: '\(originalText)' â†’ '\(improvedText)'")
          } else {
            print("âœ… [STTTranscriberManager] AI íŒë‹¨: ìˆ˜ì • ë¶ˆí•„ìš”")
          }

          // ë””ë²„ê·¸ ëª¨ë“œ: ì›ë³¸ê³¼ ê°œì„ ë³¸ì„ í•¨ê»˜ í‘œì‹œ
          if debugMode && hasChanged {
            finalized += AttributedString("[ì›ë³¸: \(originalText)] \(improvedText)\n")
          } else {
            finalized += AttributedString(improvedText)
          }

          // ìµœê·¼ ë§¥ë½ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 5ë¬¸ì¥)
          recentContextSentences.append(improvedText)
          if recentContextSentences.count > maxContextSentences {
            recentContextSentences.removeFirst()
          }

          volatile = AttributedString("")
          print("ğŸ“ [STTTranscriberManager] ìµœì¢… ì¶œë ¥: '\(improvedText)'")
        } else {
          // Partial ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ í‘œì‹œ (ì‹¤ì‹œê°„ì„± ìœ ì§€)
          volatile = result.text
          print("â³ [STTTranscriberManager] Partial text: '\(String(result.text.characters))'")
        }

        let newTranscript = String(finalized.characters) + String(volatile.characters)
        self.objectWillChange.send()
        self.transcript = newTranscript
        print("âœ… [STTTranscriberManager] Transcript updated (length \(newTranscript.count))")
      }
      print("âš ï¸ [STTTranscriberManager] Transcription loop ended")
    } catch {
      print("âŒ [STTTranscriberManager] Transcription error: \(error.localizedDescription)")
      self.objectWillChange.send()
      self.errorMessage = "ì „ì‚¬ ì˜¤ë¥˜: \(error.localizedDescription)"
    }

    isTranscribing = false
  }

  /// Process audio buffer and send to transcriber
  func processAudio(buffer: AVAudioPCMBuffer) {
    guard isTranscribing else {
      print("âš ï¸ [STTTranscriberManager] Not transcribing, ignoring buffer")
      return
    }

    guard let analyzerFormat = analyzerFormat else {
      print("âš ï¸ [STTTranscriberManager] No analyzer format, ignoring buffer")
      return
    }

    print("ğŸ¤ [STTTranscriberManager] Received audio buffer: \(buffer.frameLength) frames at \(buffer.format.sampleRate)Hz, \(buffer.format.channelCount) channels")

    // Convert format if needed
    let sendBuffer: AVAudioPCMBuffer

    let needsConversion = buffer.format.sampleRate != analyzerFormat.sampleRate ||
                         buffer.format.channelCount != analyzerFormat.channelCount

    if needsConversion {
      print("ğŸ”„ [STTTranscriberManager] Converting from \(buffer.format.sampleRate)Hz/\(buffer.format.channelCount)ch to \(analyzerFormat.sampleRate)Hz/\(analyzerFormat.channelCount)ch")

      if converter == nil || converter?.inputFormat != buffer.format || converter?.outputFormat != analyzerFormat {
        converter = AVAudioConverter(from: buffer.format, to: analyzerFormat)
        converter?.primeMethod = .none
        print("âœ… [STTTranscriberManager] Created new converter")
      }

      guard let converter = converter else {
        print("âŒ [STTTranscriberManager] Failed to create converter")
        return
      }

      // Calculate output frame capacity
      let outputFrameCapacity = AVAudioFrameCount(ceil(Double(buffer.frameLength) * analyzerFormat.sampleRate / buffer.format.sampleRate))

      guard let out = AVAudioPCMBuffer(pcmFormat: analyzerFormat, frameCapacity: outputFrameCapacity) else {
        print("âŒ [STTTranscriberManager] Failed to create output buffer")
        return
      }

      var err: NSError?
      let status = converter.convert(to: out, error: &err) { _, inStatus in
        inStatus.pointee = .haveData
        return buffer
      }

      if let err = err {
        print("âŒ [STTTranscriberManager] AVAudioConverter error: \(err)")
        return
      }

      if status == .error {
        print("âŒ [STTTranscriberManager] Conversion failed with error status")
        return
      }

      // Verify frameLength
      guard out.frameLength > 0 else {
        print("âŒ [STTTranscriberManager] Converted buffer has zero frames")
        return
      }

      print("âœ… [STTTranscriberManager] Converted successfully: \(out.frameLength) frames")
      sendBuffer = out
    } else {
      print("âœ… [STTTranscriberManager] No conversion needed, using original buffer")
      sendBuffer = buffer
    }

    // Send to both analyzers
    koInputContinuation?.yield(AnalyzerInput(buffer: sendBuffer))
    enInputContinuation?.yield(AnalyzerInput(buffer: sendBuffer))
    print("âœ… [STTTranscriberManager] Audio buffer sent to both analyzers (\(sendBuffer.frameLength) frames)")
  }

  /// Stop transcription
  func stopTranscription() {
    print("ğŸ›‘ [STTTranscriberManager] Stopping dual transcription...")

    // Finish both input streams
    koInputContinuation?.finish()
    enInputContinuation?.finish()

    // Clear all resources
    koInputContinuation = nil
    enInputContinuation = nil
    koAnalyzer = nil
    enAnalyzer = nil
    analyzerFormat = nil
    converter = nil
    koTranscriber = nil
    enTranscriber = nil
    isTranscribing = false
    recentContextSentences.removeAll()

    // Cleanup Foundation Models
    if #available(macOS 15.1, *) {
      STTFoundationModels.shared.cleanup()
    }

    print("âœ… [STTTranscriberManager] Dual transcription stopped")
  }

  /// Clear transcript
  func clearTranscript() {
    transcript = ""
    errorMessage = nil
    recentContextSentences.removeAll()
  }

  /// AI í…ìŠ¤íŠ¸ ê°œì„  ê¸°ëŠ¥ ì¼œê¸°/ë„ê¸°
  func setAIImprovement(enabled: Bool) {
    enableAIImprovement = enabled
    print("ğŸ”§ [STTTranscriberManager] AI improvement \(enabled ? "enabled" : "disabled")")
  }

  /// ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°/ë„ê¸°
  func setDebugMode(enabled: Bool) {
    debugMode = enabled
    print("ğŸ”§ [STTTranscriberManager] Debug mode \(enabled ? "enabled" : "disabled")")
  }

  /// ìë™ ì–¸ì–´ ê°ì§€ ì¼œê¸°/ë„ê¸°
  func setLanguageDetection(enabled: Bool) {
    languageDetectionEnabled = enabled
    print("ğŸ”§ [STTTranscriberManager] Language detection \(enabled ? "enabled (auto)" : "disabled (Korean only)")")
  }

  deinit {
    stopTranscription()
  }

  /// Timeout helper
  private func withTimeout<T>(seconds: TimeInterval, operation: @escaping () async -> T) async -> T? {
    await withTaskGroup(of: T?.self) { group in
      group.addTask {
        await operation()
      }

      group.addTask {
        try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
        return nil
      }

      let result = await group.next()
      group.cancelAll()
      return result ?? nil
    }
  }
}
